{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas            as pd\n",
    "import seaborn           as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import unidecode\n",
    "\n",
    "from sklearn.model_selection         import train_test_split\n",
    "from sklearn.linear_model            import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk                            import tokenize\n",
    "from string                          import punctuation\n",
    "\n",
    "# Criando variável com os dados de teste.\n",
    "data = pd.read_csv(\"C:/Users/lucas/Documents/GitHub/NLTK_IA/imdb-reviews-pt-br.csv\")\n",
    "\n",
    "# Criando as variáveis com as informações segregadas.\n",
    "treino, teste, class_treino, class_teste = train_test_split(data.text_pt, data.sentiment, random_state = 42)\n",
    "\n",
    "# Trocando as informações da coluna, para os valores legiveis pela IA.\n",
    "classificacao = data[\"sentiment\"].replace([\"neg\", \"pos\"], [0, 1])\n",
    "data[\"classificacao\"] = classificacao\n",
    "\n",
    "# Retirada dos acentos das palavras.\n",
    "data[\"text_pt\"] = [unidecode.unidecode(texto) for texto in data[\"text_pt\"]]\n",
    "\n",
    "# Define que a variável palavras receba toda a base de dados a serem usados.\n",
    "palavras = ''.join([texto for texto in data.text_pt])\n",
    "\n",
    "# Configura a vetorização do Tfidf\n",
    "tfidf = TfidfVectorizer(lowercase=False, max_features=50)\n",
    "\n",
    "# Possibilita seprar as palavras, a partir de espaçoes em branco.\n",
    "token_espaco    = tokenize.WhitespaceTokenizer()\n",
    "token_pontuacao = tokenize.WordPunctTokenizer()\n",
    "\n",
    "# Cria uma lista de pontuações.\n",
    "pontuacao = list()\n",
    "for ponto in punctuation:\n",
    "    pontuacao.append(ponto)\n",
    "# Atribui a variável a biblioteca RSLPStemmer da nltk.\n",
    "stemmer = nltk.RSLPStemmer()\n",
    "    \n",
    "# Cria uma lista de palavras indesejadas\n",
    "palavras_irrelevantes = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "# Cria uma lista de palavras indesejadas sem acentos\n",
    "palavras_irrelevantes_sem_acentos = [unidecode.unidecode(texto) for texto in palavras_irrelevantes]\n",
    "# Cria uma variável com tudo de irrelevante para a IA\n",
    "stopWords_pontuacao_acentos = pontuacao + palavras_irrelevantes + [unidecode.unidecode(texto) for texto in palavras_irrelevantes];\n",
    "\n",
    "# Classificação das palavras, primeira apuração de precisão da IA.\n",
    "# Compara os textos e retorna uma acurácia\n",
    "def class_texto(texto, coluna_texto, coluna_classificacao):\n",
    "    vetorizar = CountVectorizer(lowercase=False, max_features=50)\n",
    "    bag_of_words = vetorizar.fit_transform(texto[coluna_texto])\n",
    "    treino, teste, class_treino, class_teste = train_test_split(bag_of_words,\n",
    "                                                                texto[coluna_classificacao],\n",
    "                                                                random_state = 42)\n",
    "\n",
    "    # Absorção de toda a base de dados use: LogisticRegression(max_iter=1000)\n",
    "    reg_log = LogisticRegression()\n",
    "    reg_log.fit(treino, class_treino)\n",
    "    return reg_log.score(teste, class_teste)\n",
    "\n",
    "# Produz um gráfico que indica a quantidaede de vezes que uma palavra foi repetida.\n",
    "def pareto (texto, coluna_texto, quantidade):\n",
    "    palavras = ' '.join([texto for texto in texto[coluna_texto]])\n",
    "    # Aplica o token criando anteriormente para todas as palavras\n",
    "    token_frase = token_espaco.tokenize(palavras)\n",
    "    # Cria uma frequencia para todas as lapavras, individualemnte\n",
    "    frequencia = nltk.FreqDist(token_frase)\n",
    "    # Inicia uma visualização para cada uma dessas palavras, de acordo com sua uantidade de aparições\n",
    "    df_frequencia = pd.DataFrame({\"Palavra\": list(frequencia.keys()),\"Frequencia\": list(frequencia.values())})\n",
    "    # Limita para quantidade X de linhas\n",
    "    maisVistas = df_frequencia.nlargest(columns = \"Frequencia\", n = quantidade)\n",
    "    # Configuração da umagem a ser exibida\n",
    "    plt.figure(figsize=(15,10))\n",
    "\n",
    "    ax = sns.barplot(data = maisVistas, x = \"Palavra\", y = \"Frequencia\", color = 'gray')\n",
    "    ax.set(ylabel = 'Contagem')\n",
    "    plt.show()\n",
    "\n",
    "def tratarMaiusculas(palavras, palavras_indesejadas, nova_coluna):\n",
    "    frase_processada = list()\n",
    "    for opiniao in palavras:\n",
    "        nova_frase = list()\n",
    "        opiniao = opiniao.lower()\n",
    "        palavras_opiniao = token_pontuacao.tokenize(opiniao)\n",
    "        for palavra in palavras_opiniao:\n",
    "            if palavra not in palavras_indesejadas:\n",
    "                nova_frase.append(palavra)\n",
    "        frase_processada.append(' '.join(nova_frase))\n",
    "    data[nova_coluna] = frase_processada\n",
    "    \n",
    "# Trata as palavras, de acordo com aquilo que deve se removido e adiciona uma nova coluna com as informações tratadas.\n",
    "def tratarEspacos(palavras, palavras_indesejadas, nova_coluna):\n",
    "    frase_processada = list()\n",
    "    for opiniao in palavras:\n",
    "        nova_frase = list()\n",
    "        palavras_opiniao = token_espaco.tokenize(opiniao)\n",
    "        for palavra in palavras_opiniao:\n",
    "            if palavra not in palavras_indesejadas:\n",
    "                nova_frase.append(palavra)\n",
    "        frase_processada.append(' '.join(nova_frase))\n",
    "    data[nova_coluna] = frase_processada\n",
    "    \n",
    "# Trata as palavras, de acordo com aquilo que deve se removido e adiciona uma nova coluna com as informações tratadas.\n",
    "def tratarPontuacao(palavras, palavras_indesejadas, nova_coluna):\n",
    "    frase_processada = list()\n",
    "    for opiniao in palavras:\n",
    "        nova_frase = list()\n",
    "        palavras_opiniao = token_pontuacao.tokenize(opiniao)\n",
    "        for palavra in palavras_opiniao:\n",
    "            if palavra not in palavras_indesejadas:\n",
    "                nova_frase.append(palavra)\n",
    "        frase_processada.append(' '.join(nova_frase))\n",
    "    data[nova_coluna] = frase_processada\n",
    "    \n",
    "def tratarsufixo(palavras, palavras_indesejadas, nova_coluna):\n",
    "    frase_processada = list()\n",
    "    for opiniao in palavras:\n",
    "        nova_frase = list()\n",
    "        palavras_opiniao = token_pontuacao.tokenize(opiniao)\n",
    "        for palavra in palavras_opiniao:\n",
    "            nova_frase.append(stemmer.stem(palavra))\n",
    "        frase_processada.append(' '.join(nova_frase))\n",
    "    data[nova_coluna] = frase_processada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#palavras_neg(data, \"text_pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#palavras_pos(data, \"text_pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#palavras(data, \"text_pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamada do método que transforma todas as letras em minusculas.\n",
    "tratarMaiusculas(data.text_pt, pontuacao, \"tratamento_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamada do método que retira palavras indesejadas.\n",
    "tratarEspacos(data.tratamento_1, palavras_irrelevantes, \"tratamento_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamada do método que retira pontuação desnecessária.\n",
    "tratarPontuacao(data.tratamento_2, stopWords_pontuacao_acentos, \"tratamento_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamada do método que retira sufixos.\n",
    "tratarsufixo(data.tratamento_3, stopWords_pontuacao_acentos, \"tratamento_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6988273352203801\n"
     ]
    }
   ],
   "source": [
    "vetorizar = CountVectorizer(lowercase=False, max_features=50)\n",
    "bag_of_words = tfidf.fit_transform(data[\"tratamento_4\"])\n",
    "treino, teste, class_treino, class_teste = train_test_split(bag_of_words,\n",
    "                                                            data[\"classificacao\"],\n",
    "                                                            random_state = 42)\n",
    "\n",
    "# Absorção de toda a base de dados use: LogisticRegression(max_iter=1000)\n",
    "reg_log = LogisticRegression()\n",
    "reg_log.fit(treino, class_treino)\n",
    "acuracia_tfidf = reg_log.score(teste, class_teste)\n",
    "print(acuracia_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8926000808734331\n"
     ]
    }
   ],
   "source": [
    "vetorizar = CountVectorizer(lowercase=False, ngram_range = (1,2))\n",
    "bag_of_words = vetorizar.fit_transform(data[\"tratamento_4\"])\n",
    "treino, teste, class_treino, class_teste = train_test_split(bag_of_words,\n",
    "                                                            data[\"classificacao\"],\n",
    "                                                            random_state = 42)\n",
    "reg_log = LogisticRegression(max_iter=1000)\n",
    "reg_log.fit(treino, class_treino)\n",
    "acuracia_tfidf = reg_log.score(teste, class_teste)\n",
    "print(acuracia_tfidf)\n",
    "\n",
    "#frase = [\"Esse filme não é bom\"]\n",
    "#previsao = reg_log.predict([frase])\n",
    "#print(previsao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
