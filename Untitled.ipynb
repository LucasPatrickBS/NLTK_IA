{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas            as pd\n",
    "import seaborn           as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import unidecode\n",
    "\n",
    "from sklearn.model_selection         import train_test_split\n",
    "from sklearn.linear_model            import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk                            import tokenize\n",
    "from string                          import punctuation\n",
    "\n",
    "# Criando variável com os dados de teste.\n",
    "data = pd.read_csv(\"C:/Users/lucas/Documents/GitHub/NLTK_IA/imdb-reviews-pt-br.csv\")\n",
    "\n",
    "# Criando as variáveis com as informações segregadas.\n",
    "treino, teste, class_treino, class_teste = train_test_split(data.text_pt, data.sentiment, random_state = 42)\n",
    "\n",
    "# Trocando as informações da coluna, para os valores legiveis pela IA.\n",
    "classificacao = data[\"sentiment\"].replace([\"neg\", \"pos\"], [0, 1])\n",
    "data[\"classificacao\"] = classificacao\n",
    "\n",
    "# Retirada dos acentos das palavras.\n",
    "data[\"text_pt\"] = [unidecode.unidecode(texto) for texto in data[\"text_pt\"]]\n",
    "\n",
    "# Define que a variável palavras receba toda a base de dados a serem usados.\n",
    "palavras = ''.join([texto for texto in data.text_pt])\n",
    "\n",
    "# Configura a vetorização do Tfidf\n",
    "tfidf = TfidfVectorizer(lowercase=False, max_features=50)\n",
    "\n",
    "# Possibilita seprar as palavras, a partir de espaçoes em branco.\n",
    "token_espaco    = tokenize.WhitespaceTokenizer()\n",
    "token_pontuacao = tokenize.WordPunctTokenizer()\n",
    "\n",
    "# Cria uma lista de pontuações.\n",
    "pontuacao = list()\n",
    "for ponto in punctuation:\n",
    "    pontuacao.append(ponto)\n",
    "# Atribui a variável a biblioteca RSLPStemmer da nltk.\n",
    "stemmer = nltk.RSLPStemmer()\n",
    "    \n",
    "# Cria uma lista de palavras indesejadas\n",
    "palavras_irrelevantes = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "# Cria uma lista de palavras indesejadas sem acentos\n",
    "palavras_irrelevantes_sem_acentos = [unidecode.unidecode(texto) for texto in palavras_irrelevantes]\n",
    "# Cria uma variável com tudo de irrelevante para a IA\n",
    "stopWords_pontuacao_acentos = pontuacao + palavras_irrelevantes + [unidecode.unidecode(texto) for texto in palavras_irrelevantes];\n",
    "\n",
    "# Classificação das palavras, primeira apuração de precisão da IA.\n",
    "# Compara os textos e retorna uma acurácia\n",
    "def class_texto(texto, coluna_texto, coluna_classificacao):\n",
    "    vetorizar = CountVectorizer(lowercase=False, max_features=50)\n",
    "    bag_of_words = vetorizar.fit_transform(texto[coluna_texto])\n",
    "    treino, teste, class_treino, class_teste = train_test_split(bag_of_words,\n",
    "                                                                texto[coluna_classificacao],\n",
    "                                                                random_state = 42)\n",
    "\n",
    "    # Absorção de toda a base de dados use: LogisticRegression(max_iter=1000)\n",
    "    reg_log = LogisticRegression()\n",
    "    reg_log.fit(treino, class_treino)\n",
    "    return reg_log.score(teste, class_teste)\n",
    "\n",
    "# Produz um gráfico que indica a quantidaede de vezes que uma palavra foi repetida.\n",
    "def pareto (texto, coluna_texto, quantidade):\n",
    "    palavras = ' '.join([texto for texto in texto[coluna_texto]])\n",
    "    # Aplica o token criando anteriormente para todas as palavras\n",
    "    token_frase = token_espaco.tokenize(palavras)\n",
    "    # Cria uma frequencia para todas as lapavras, individualemnte\n",
    "    frequencia = nltk.FreqDist(token_frase)\n",
    "    # Inicia uma visualização para cada uma dessas palavras, de acordo com sua uantidade de aparições\n",
    "    df_frequencia = pd.DataFrame({\"Palavra\": list(frequencia.keys()),\"Frequencia\": list(frequencia.values())})\n",
    "    # Limita para quantidade X de linhas\n",
    "    maisVistas = df_frequencia.nlargest(columns = \"Frequencia\", n = quantidade)\n",
    "    # Configuração da umagem a ser exibida\n",
    "    plt.figure(figsize=(15,10))\n",
    "\n",
    "    ax = sns.barplot(data = maisVistas, x = \"Palavra\", y = \"Frequencia\", color = 'gray')\n",
    "    ax.set(ylabel = 'Contagem')\n",
    "    plt.show()\n",
    "\n",
    "def tratarMaiusculas(palavras, palavras_indesejadas, nova_coluna):\n",
    "    frase_processada = list()\n",
    "    for opiniao in palavras:\n",
    "        nova_frase = list()\n",
    "        opiniao = opiniao.lower()\n",
    "        palavras_opiniao = token_pontuacao.tokenize(opiniao)\n",
    "        for palavra in palavras_opiniao:\n",
    "            if palavra not in palavras_indesejadas:\n",
    "                nova_frase.append(palavra)\n",
    "        frase_processada.append(' '.join(nova_frase))\n",
    "    data[nova_coluna] = frase_processada\n",
    "    \n",
    "# Trata as palavras, de acordo com aquilo que deve se removido e adiciona uma nova coluna com as informações tratadas.\n",
    "def tratarEspacos(palavras, palavras_indesejadas, nova_coluna):\n",
    "    frase_processada = list()\n",
    "    for opiniao in palavras:\n",
    "        nova_frase = list()\n",
    "        palavras_opiniao = token_espaco.tokenize(opiniao)\n",
    "        for palavra in palavras_opiniao:\n",
    "            if palavra not in palavras_indesejadas:\n",
    "                nova_frase.append(palavra)\n",
    "        frase_processada.append(' '.join(nova_frase))\n",
    "    data[nova_coluna] = frase_processada\n",
    "    \n",
    "# Trata as palavras, de acordo com aquilo que deve se removido e adiciona uma nova coluna com as informações tratadas.\n",
    "def tratarPontuacao(palavras, palavras_indesejadas, nova_coluna):\n",
    "    frase_processada = list()\n",
    "    for opiniao in palavras:\n",
    "        nova_frase = list()\n",
    "        palavras_opiniao = token_pontuacao.tokenize(opiniao)\n",
    "        for palavra in palavras_opiniao:\n",
    "            if palavra not in palavras_indesejadas:\n",
    "                nova_frase.append(palavra)\n",
    "        frase_processada.append(' '.join(nova_frase))\n",
    "    data[nova_coluna] = frase_processada\n",
    "    \n",
    "def tratarsufixo(palavras, palavras_indesejadas, nova_coluna):\n",
    "    frase_processada = list()\n",
    "    for opiniao in palavras:\n",
    "        nova_frase = list()\n",
    "        palavras_opiniao = token_pontuacao.tokenize(opiniao)\n",
    "        for palavra in palavras_opiniao:\n",
    "            nova_frase.append(stemmer.stem(palavra))\n",
    "        frase_processada.append(' '.join(nova_frase))\n",
    "    data[nova_coluna] = frase_processada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#palavras_neg(data, \"text_pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#palavras_pos(data, \"text_pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#palavras(data, \"text_pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamada do método que transforma todas as letras em minusculas.\n",
    "tratarMaiusculas(data.text_pt, pontuacao, \"tratamento_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamada do método que retira palavras indesejadas.\n",
    "tratarEspacos(data.tratamento_1, palavras_irrelevantes, \"tratamento_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamada do método que retira pontuação desnecessária.\n",
    "tratarPontuacao(data.tratamento_2, stopWords_pontuacao_acentos, \"tratamento_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamada do método que retira sufixos.\n",
    "tratarsufixo(data.tratamento_3, stopWords_pontuacao_acentos, \"tratamento_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6988273352203801\n"
     ]
    }
   ],
   "source": [
    "vetorizar = CountVectorizer(lowercase=False, max_features=50)\n",
    "bag_of_words = tfidf.fit_transform(data[\"tratamento_4\"])\n",
    "treino, teste, class_treino, class_teste = train_test_split(bag_of_words,\n",
    "                                                            data[\"classificacao\"],\n",
    "                                                            random_state = 42)\n",
    "\n",
    "# Absorção de toda a base de dados use: LogisticRegression(max_iter=1000)\n",
    "reg_log = LogisticRegression()\n",
    "reg_log.fit(treino, class_treino)\n",
    "acuracia_tfidf = reg_log.score(teste, class_teste)\n",
    "print(acuracia_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "vetorizar = CountVectorizer(lowercase=False, ngram_range = (1,2))\n",
    "bag_of_words = vetorizar.fit_transform(data[\"tratamento_4\"])\n",
    "treino, teste, class_treino, class_teste = train_test_split(bag_of_words,\n",
    "                                                            data[\"classificacao\"],\n",
    "                                                            random_state = 42)\n",
    "reg_log = LogisticRegression(max_iter=1000)\n",
    "reg_log.fit(treino, class_treino)\n",
    "# acuracia_tfidf = reg_log.score(teste, class_teste)\n",
    "# print(acuracia_tfidf)\n",
    "\n",
    "previsao = reg_log.predict(treino)\n",
    "print(previsao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Este filme foi bastante horrivel Vipul Shahs ultimo filme foi bom este foi apenas ruim, embora seja uma boa historia e e tratado de uma forma otima Aatish Kapadia que adaptou este filme de outra peca gujarati \"Avjo Wahala Fari Malishu\" fez um bom, mas lento pianful 2 horas e meia para assistir ha um monte de falhas neste filme, mas ainda e um artista musicas sao um pouco desanimado e nao funcionam bem, mas eles ainda sao bons em geral, nao um filme que voce entusiasticamente assistir a sua ainda uma historia para levar para conta e e bom se voce e o tipo de relacionamento filme muito bom com um monte de falhas e humor isso realmente nao e necessario nem um pouco'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text_pt'][40499] # Negativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mais uma vez, o Sr. Costner arrumou um filme por muito mais tempo do que o necessario. Alem das terriveis sequencias de resgate no mar, das quais ha muito poucas, eu simplesmente nao me importei com nenhum dos personagens. A maioria de nos tem fantasmas no armario, e o personagem Costers e realizado logo no inicio, e depois esquecido ate muito mais tarde, quando eu nao me importava. O personagem com o qual deveriamos nos importar e muito arrogante e superconfiante, Ashton Kutcher. O problema e que ele sai como um garoto que pensa que e melhor do que qualquer outra pessoa ao seu redor e nao mostra sinais de um armario desordenado. Seu unico obstaculo parece estar vencendo Costner. Finalmente, quando estamos bem alem do meio do caminho, Costner nos conta sobre os fantasmas dos Kutchers. Somos informados de por que Kutcher e levado a ser o melhor sem pressentimentos ou pressagios anteriores. Nenhuma magica aqui, era tudo que eu podia fazer para nao desligar uma hora.'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text_pt'][0] # Positva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicionario = {\n",
    "    'frases': ['Mais uma vez, o Sr. Costner arrumou um filme por muito mais tempo do que o necessario. Alem das terriveis sequencias de resgate no mar, das quais ha muito poucas, eu simplesmente nao me importei com nenhum dos personagens. A maioria de nos tem fantasmas no armario, e o personagem Costers e realizado logo no inicio, e depois esquecido ate muito mais tarde, quando eu nao me importava. O personagem com o qual deveriamos nos importar e muito arrogante e superconfiante, Ashton Kutcher. O problema e que ele sai como um garoto que pensa que e melhor do que qualquer outra pessoa ao seu redor e nao mostra sinais de um armario desordenado. Seu unico obstaculo parece estar vencendo Costner. Finalmente, quando estamos bem alem do meio do caminho, Costner nos conta sobre os fantasmas dos Kutchers. Somos informados de por que Kutcher e levado a ser o melhor sem pressentimentos ou pressagios anteriores. Nenhuma magica aqui, era tudo que eu podia fazer para nao desligar uma hora.', 'Este filme foi bastante horrivel Vipul Shahs ultimo filme foi bom este foi apenas ruim, embora seja uma boa historia e e tratado de uma forma otima Aatish Kapadia que adaptou este filme de outra peca gujarati \"Avjo Wahala Fari Malishu\" fez um bom, mas lento pianful 2 horas e meia para assistir ha um monte de falhas neste filme, mas ainda e um artista musicas sao um pouco desanimado e nao funcionam bem, mas eles ainda sao bons em geral, nao um filme que voce entusiasticamente assistir a sua ainda uma historia para levar para conta e e bom se voce e o tipo de relacionamento filme muito bom com um monte de falhas e humor isso realmente nao e necessario nem um pouco'],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dicionario, columns=['frases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "vetorizar = CountVectorizer(lowercase=False, ngram_range = (1,2))\n",
    "bag_of_words = vetorizar.fit_transform(data[\"tratamento_4\"])\n",
    "treino, teste, class_treino, class_teste = train_test_split(bag_of_words,\n",
    "                                                            data[\"classificacao\"],\n",
    "                                                            random_state = 42)\n",
    "\n",
    "bag_of_words = vetorizar.transform(df[\"frases\"])\n",
    "\n",
    "reg_log = LogisticRegression(max_iter=1000)\n",
    "reg_log.fit(treino, class_treino)\n",
    "previsao = reg_log.predict(bag_of_words)\n",
    "print(previsao)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
