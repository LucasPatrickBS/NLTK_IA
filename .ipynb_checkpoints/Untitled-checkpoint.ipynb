{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas            as pd\n",
    "import seaborn           as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import unidecode\n",
    "\n",
    "from sklearn.model_selection         import train_test_split\n",
    "from sklearn.linear_model            import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk                            import tokenize\n",
    "from string                          import punctuation\n",
    "\n",
    "# Criando variável com os dados de teste.\n",
    "data = pd.read_csv(\"C:/Users/lucas/Documents/GitHub/NLTK_IA/imdb-reviews-pt-br.csv\")\n",
    "\n",
    "# Criando as variáveis com as informações segregadas.\n",
    "treino, teste, class_treino, class_teste = train_test_split(data.text_pt, data.sentiment, random_state = 42)\n",
    "\n",
    "# Trocando as informações da coluna, para os valores legiveis pela IA.\n",
    "classificacao = data[\"sentiment\"].replace([\"neg\", \"pos\"], [0, 1])\n",
    "data[\"classificacao\"] = classificacao\n",
    "\n",
    "# Retirada dos acentos das palavras.\n",
    "data[\"text_pt\"] = [unidecode.unidecode(texto) for texto in data[\"text_pt\"]]\n",
    "\n",
    "# Define que a variável palavras receba toda a base de dados a serem usados.\n",
    "palavras = ''.join([texto for texto in data.text_pt])\n",
    "\n",
    "# Configura a vetorização do Tfidf\n",
    "tfidf = TfidfVectorizer(lowercase=False, max_features=50)\n",
    "\n",
    "# Possibilita seprar as palavras, a partir de espaçoes em branco.\n",
    "token_espaco    = tokenize.WhitespaceTokenizer()\n",
    "token_pontuacao = tokenize.WordPunctTokenizer()\n",
    "\n",
    "# Cria uma lista de pontuações.\n",
    "pontuacao = list()\n",
    "for ponto in punctuation:\n",
    "    pontuacao.append(ponto)\n",
    "# Atribui a variável a biblioteca RSLPStemmer da nltk.\n",
    "stemmer = nltk.RSLPStemmer()\n",
    "    \n",
    "# Cria uma lista de palavras indesejadas\n",
    "palavras_irrelevantes = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "# Cria uma lista de palavras indesejadas sem acentos\n",
    "palavras_irrelevantes_sem_acentos = [unidecode.unidecode(texto) for texto in palavras_irrelevantes]\n",
    "# Cria uma variável com tudo de irrelevante para a IA\n",
    "stopWords_pontuacao_acentos = pontuacao + palavras_irrelevantes + [unidecode.unidecode(texto) for texto in palavras_irrelevantes];\n",
    "\n",
    "# Classificação das palavras, primeira apuração de precisão da IA.\n",
    "# Compara os textos e retorna uma acurácia\n",
    "def class_texto(texto, coluna_texto, coluna_classificacao):\n",
    "    vetorizar = CountVectorizer(lowercase=False, max_features=50)\n",
    "    bag_of_words = vetorizar.fit_transform(texto[coluna_texto])\n",
    "    treino, teste, class_treino, class_teste = train_test_split(bag_of_words,\n",
    "                                                                texto[coluna_classificacao],\n",
    "                                                                random_state = 42)\n",
    "\n",
    "    # Absorção de toda a base de dados use: LogisticRegression(max_iter=1000)\n",
    "    reg_log = LogisticRegression()\n",
    "    reg_log.fit(treino, class_treino)\n",
    "    return reg_log.score(teste, class_teste)\n",
    "\n",
    "# Produz um gráfico que indica a quantidaede de vezes que uma palavra foi repetida.\n",
    "def pareto (texto, coluna_texto, quantidade):\n",
    "    palavras = ' '.join([texto for texto in texto[coluna_texto]])\n",
    "    # Aplica o token criando anteriormente para todas as palavras\n",
    "    token_frase = token_espaco.tokenize(palavras)\n",
    "    # Cria uma frequencia para todas as lapavras, individualemnte\n",
    "    frequencia = nltk.FreqDist(token_frase)\n",
    "    # Inicia uma visualização para cada uma dessas palavras, de acordo com sua uantidade de aparições\n",
    "    df_frequencia = pd.DataFrame({\"Palavra\": list(frequencia.keys()),\"Frequencia\": list(frequencia.values())})\n",
    "    # Limita para quantidade X de linhas\n",
    "    maisVistas = df_frequencia.nlargest(columns = \"Frequencia\", n = quantidade)\n",
    "    # Configuração da umagem a ser exibida\n",
    "    plt.figure(figsize=(15,10))\n",
    "\n",
    "    ax = sns.barplot(data = maisVistas, x = \"Palavra\", y = \"Frequencia\", color = 'gray')\n",
    "    ax.set(ylabel = 'Contagem')\n",
    "    plt.show()\n",
    "\n",
    "def tratarMaiusculas(palavras, palavras_indesejadas, nova_coluna):\n",
    "    frase_processada = list()\n",
    "    for opiniao in palavras:\n",
    "        nova_frase = list()\n",
    "        opiniao = opiniao.lower()\n",
    "        palavras_opiniao = token_pontuacao.tokenize(opiniao)\n",
    "        for palavra in palavras_opiniao:\n",
    "            if palavra not in palavras_indesejadas:\n",
    "                nova_frase.append(palavra)\n",
    "        frase_processada.append(' '.join(nova_frase))\n",
    "    data[nova_coluna] = frase_processada\n",
    "    \n",
    "# Trata as palavras, de acordo com aquilo que deve se removido e adiciona uma nova coluna com as informações tratadas.\n",
    "def tratarEspacos(palavras, palavras_indesejadas, nova_coluna):\n",
    "    frase_processada = list()\n",
    "    for opiniao in palavras:\n",
    "        nova_frase = list()\n",
    "        palavras_opiniao = token_espaco.tokenize(opiniao)\n",
    "        for palavra in palavras_opiniao:\n",
    "            if palavra not in palavras_indesejadas:\n",
    "                nova_frase.append(palavra)\n",
    "        frase_processada.append(' '.join(nova_frase))\n",
    "    data[nova_coluna] = frase_processada\n",
    "    \n",
    "# Trata as palavras, de acordo com aquilo que deve se removido e adiciona uma nova coluna com as informações tratadas.\n",
    "def tratarPontuacao(palavras, palavras_indesejadas, nova_coluna):\n",
    "    frase_processada = list()\n",
    "    for opiniao in palavras:\n",
    "        nova_frase = list()\n",
    "        palavras_opiniao = token_pontuacao.tokenize(opiniao)\n",
    "        for palavra in palavras_opiniao:\n",
    "            if palavra not in palavras_indesejadas:\n",
    "                nova_frase.append(palavra)\n",
    "        frase_processada.append(' '.join(nova_frase))\n",
    "    data[nova_coluna] = frase_processada\n",
    "    \n",
    "def tratarsufixo(palavras, palavras_indesejadas, nova_coluna):\n",
    "    frase_processada = list()\n",
    "    for opiniao in palavras:\n",
    "        nova_frase = list()\n",
    "        palavras_opiniao = token_pontuacao.tokenize(opiniao)\n",
    "        for palavra in palavras_opiniao:\n",
    "            nova_frase.append(stemmer.stem(palavra))\n",
    "        frase_processada.append(' '.join(nova_frase))\n",
    "    data[nova_coluna] = frase_processada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#palavras_neg(data, \"text_pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#palavras_pos(data, \"text_pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#palavras(data, \"text_pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamada do método que transforma todas as letras em minusculas.\n",
    "tratarMaiusculas(data.text_pt, pontuacao, \"tratamento_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamada do método que retira palavras indesejadas.\n",
    "tratarEspacos(data.tratamento_1, palavras_irrelevantes, \"tratamento_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamada do método que retira pontuação desnecessária.\n",
    "tratarPontuacao(data.tratamento_2, stopWords_pontuacao_acentos, \"tratamento_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamada do método que retira sufixos.\n",
    "tratarsufixo(data.tratamento_3, stopWords_pontuacao_acentos, \"tratamento_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6988273352203801\n"
     ]
    }
   ],
   "source": [
    "vetorizar = CountVectorizer(lowercase=False, max_features=50)\n",
    "bag_of_words = tfidf.fit_transform(data[\"tratamento_4\"])\n",
    "treino, teste, class_treino, class_teste = train_test_split(bag_of_words,\n",
    "                                                            data[\"classificacao\"],\n",
    "                                                            random_state = 42)\n",
    "\n",
    "# Absorção de toda a base de dados use: LogisticRegression(max_iter=1000)\n",
    "reg_log = LogisticRegression()\n",
    "reg_log.fit(treino, class_treino)\n",
    "acuracia_tfidf = reg_log.score(teste, class_teste)\n",
    "print(acuracia_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got scalar array instead:\narray=Esse filme é ruim, eu não gostei dele..\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-23d823ca0f1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mreg_log\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mreg_log\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtreino\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_treino\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mcalculo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg_log\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Esse filme é ruim, eu não gostei dele.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcalculo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#acuracia_tfidf = reg_log.score(teste, class_teste)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lucas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    307\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m         \"\"\"\n\u001b[1;32m--> 309\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lucas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lucas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lucas\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    628\u001b[0m             \u001b[1;31m# If input is scalar raise error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 630\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    631\u001b[0m                     \u001b[1;34m\"Expected 2D array, got scalar array instead:\\narray={}.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=Esse filme é ruim, eu não gostei dele..\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "vetorizar = CountVectorizer(lowercase=False, ngram_range = (1,2))\n",
    "bag_of_words = vetorizar.fit_transform(data[\"tratamento_4\"])\n",
    "treino, teste, class_treino, class_teste = train_test_split(bag_of_words,\n",
    "                                                            data[\"classificacao\"],\n",
    "                                                            random_state = 42)\n",
    "reg_log = LogisticRegression(max_iter=1000)\n",
    "reg_log.fit(treino, class_treino)\n",
    "calculo = reg_log.predict(\"Esse filme é ruim, eu não gostei dele.\")\n",
    "print(calculo)\n",
    "#acuracia_tfidf = reg_log.score(teste, class_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
